\section{Value Iteration Theorem [35 pts]}



In this problem, we will deal with contractions and fixed points and prove an important result from the value iteration theorem.
From lecture, we know that the Bellman backup operator $B$ given below is a contraction with the fixed point as $V^*$, the optimal value function of the MDP. The symbols have their usual meanings. $\gamma$ is the discount factor and $0 \leq \gamma < 1$. In all parts, $||v||$ is the infinity norm of the vector.
\begin{equation}
    (BV)(s) = \max_a[ R(s, a) + \gamma\sum_{s' \in S}p(s'|s,a)V(s')]
\end{equation}

We also saw the contraction operator $B_\pi$ which is the Bellman backup operator for a particular policy given below:
\begin{equation}
    (B_\pi V)(s) =  \E_{a \sim \pi}[R(s,a) + \gamma\sum_{s' \in S}p(s'|s,a)V(s')]
\end{equation}

\begin{enumerate}[label=(\alph*)]
    \item Recall that $||BV - BV'|| \leq \gamma ||V - V'||$ for two random value functions $V$ and $V'$. Prove that $B_\pi$ is also a contraction mapping:  $||B_\pi V - B_\pi V'|| \leq \gamma ||V - V'||$. [5 pts]
    \item Prove that the fixed point for $B_\pi$ is unique. What is the fixed point of $B_\pi$? [5 pts]
\end{enumerate}

\noindent In value iteration, we repeatedly apply the Bellman backup operator $B$ to improve our value function. At the end of value iteration, we can recover a greedy policy $\pi$ from the value function using the equation below:

\begin{equation}
    \pi(s) = \argmax_{a} [{r(s,a) + \gamma\sum_{s' \in S}p(s'|s,a)V(s')}]
\end{equation}

\noindent Suppose we run value iteration for a finite number of steps to obtain a value function $V$ ($V$ has not necessarily converged to $V^*$). Say now that we evaluate our policy $\pi$ obtained using the formula above to get $V^\pi$. \textbf{Note that here and for the rest of Q2, $\pi$ refers to the greedy policy.}

\begin{enumerate}
    \item[(c)] Is $V^{\pi}$ always the same as $V$? Justify your answer. [5 pts]
\end{enumerate}

\noindent In lecture, we learned that running value iteration until a certain tolerance can bring us close to recovering the optimal value function. Let $V_{n}$ and $V_{n+1}$ be the outputs of value iteration at the $n^{th}$ and $n+1^{th}$ iterations respectively. Let  $\epsilon > 0$ and consider the point in value iteration such that $||V_{n+1} - V_{n}|| < \frac{\epsilon (1 - \gamma)}{2\gamma}$. Let $\pi$ be the greedy policy given the value function $V_{n+1}$. 

\noindent You will now prove that this policy $\pi$ is $\epsilon$-optimal. This result justifies why halting value iteration when the difference between success iterations is sufficiently small, ensures the decision policy obtained by being greedy with respect to the value function, is near-optimal. 

\noindent Precisely if 
\begin{equation}
    ||V_{n+1} - V_{n}|| < \frac{\epsilon (1 - \gamma)}{2\gamma}
\end{equation}
then, 
\begin{equation}
    ||V^{\pi} - V^{*}|| \leq \epsilon
\end{equation}


\begin{enumerate}
    \item[(d)] When $\pi$ is the greedy policy, what is the relationship between $B$ and $B_\pi$? [2 pts]
    \item[(e)] Prove that $||V^{\pi} - V_{n+1}|| \leq \epsilon /2$. \\ 
    \textbf{Hint: }Introduce an in-between term and leverage the triangle inequality. [6 pts]
    \item[(f)] Prove  $||B^kV - B^kV'|| \leq \gamma^k||V - V'||$ [3 pts]
    \item[(g)] Prove that $||V^{*} - V_{n+1}|| \leq \epsilon /2$. [7pts]\\ 
    \textbf{Hints: } Note that $||V^{*} - V_{n+1}|| = ||V^{*} + V_{n+2} - V_{n+2} - V_{n+1}||$ and you can repeatedly apply this trick. It may also be useful to leverage part (f) and recall that $V^{*}$ is the fixed point of the contraction $B$. \\
\item[(h)] Use the results from parts (e) and (g), to show that $||V^{\pi} - V^{*}|| \leq \epsilon$ [2 pts]
\end{enumerate}