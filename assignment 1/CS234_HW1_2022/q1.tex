
\section{Efficient Routing MDP [35 pts]}

You are leading a routing and planning team at a self-driving car company and have decided to model your latest urban navigation problem as an MDP. Consider the following environment (Fig. \ref{fig:fig1}). Your car must navigate along the road (gray squares) while avoiding obstacles (red squares) to reach the rider's destination (the green square). Because the road is gridlocked, your car must change lanes whenever it wishes to move forward. From any gray square, your car can either move right $\&$ up, or  right $\&$ down. For example, starting from state $3$, your car can move to state $8$ or $10$. Note that it is not be possible to reach the green square from every state. Actions are deterministic and always succeed unless they will cause you to run into an impassible barrier. The thick outer edge indicates an impassible barrier, and attempting to move in the direction of a barrier from a gray square results in your car moving up one square (e.g. taking any action from state 32 moves the car to state 31). 

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=.9\linewidth]{gridworld.png}  
  \caption{Grid World}
  \label{fig:fig1.a}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=.9\linewidth]{gridworldcar2.png}  
  \caption{A successful run in Grid World.}
  \label{fig:fig1.b}
\end{subfigure}
\caption{}
\label{fig:fig1}
\end{figure}

A successful run in Grid World 1 is shown in Figure \ref{fig:fig1.b}. Taking any action from the green destination square (no. 33) earns a reward of $r_g$  and ends the episode. Taking any action from the red squares that depict obstacles (no. 1, 7, 13...) earns a reward of $r_r$ and ends the episode. Otherwise, from every other square, taking any action is associated with a reward $r_s$. Assume the discount factor $\gamma = 0.9$, $r_g = +5$, and $r_r = -5$ unless otherwise specified. Notice the horizon is technically infinite. 

\begin{enumerate}[label=(\alph*)]
\item  Let $r_s \in \{-5,-0.5,0,2\}$. Starting in \textbf{square 2}, for each of the possible values of $r_s$, briefly explain what the optimal policy would be in Grid World. In each case is the optimal policy unique and does the optimal policy depend on the value of the discount factor $\gamma$? Explain your answer.   [5 pts]


\item Which values of $r_s \in \{-5, -0.5, 0, 2\}$ will yield a policy that returns the shortest path to the green square? (Hint: At least one does.) Explain which ones do, then, pick the minimum of this set of rewards that does, and then find the optimal value function for \textbf{states 2, 13, 21 and 32}. [5 pts]

\end{enumerate}

\noindent Rather than finding the shortest path between two points, suppose our car is low on gas, so we want to take the path that uses the least fuel. In the real world, navigation optimized for fuel consumption may take more steps to reach a destination \footnote{\href{https://blog.google/products/maps/3-new-ways-navigate-more-sustainably-maps/}{Google Maps Blog}}. 

Consider the same MDP, but with two new ``efficient actions'' -- move right or move down. For example, starting from state 3, you can either move to state 4 or 9. Once again, the actions are deterministic and always succeed unless you run into a wall. Attempting to move in the direction of a wall from a gray square using an efficient action results in you moving \textit{down} one square. For clarity, we will use separate symbols $r_s$ for the reward associated with an inefficient action (right $\&$ up, or  right $\&$ down) and $r_e$ for the reward associated with an efficient action.

\begin{enumerate}

\item[(c)] Let $r_e \in \{-5,-0.5,0,2\}$. Starting in \textbf{state 2}, for each of the possible values of $r_e$, briefly explain what the optimal policy would be in Grid World \textit{using only efficient actions}. In each case is the optimal policy unique and does the optimal policy depend on the value of the discount factor $\gamma$? Explain your answer. Which values of $r_e$ would cause the optimal policy to return the shortest path to the green destination square?  [5 pts]

\item[(d)] Consider now that $r_s = 0$. Derive a relation for $r_e$ such that the optimal path from \textbf{state 2} to the destination square using only efficient actions is strictly more rewarding than the optimal path using only inefficient actions.  [5 pts]

\item[(e)] Compare the set of gray states that can reach the goal using only efficient actions, and the set of gray states that can reach the goal using only inefficient actions. Which states are part of one set but not part of the other? Explain your answer. [5 pts]

\item [(f)] Consider a general MDP with rewards and transitions. Consider a discount factor of $\gamma$. Assume that the horizon is infinite (so there is no termination). Can adding a constant c to all rewards ($r_{new} = c + r_{old}$) change the optimal policy of the MDP? If yes, give an example for Grid World with efficient actions using the $r_g$, $r_s$ and $r_e$ such that the optimal policy changes for a specific constant. [5 pts]

\item[(g)]
Imagine your efficient routing MDP is to be used in a popular maps app or website.  Choosing what route options will be available and which will be default present inevitable value judgements. The shortest path and the sustainable path are each optimal given their choices of rewards, but the rewards formalize different values.  How would you present the shortest path and sustainable path options in a maps app and why? Please use 2-4 sentences to explain your answer. There is no single ``correct'' answer-- reasonable explanations of your choice will receive full credit. [5 pts]

Context for the debate: “Green nudges” such as setting the most efficient route as the (changeable) default on mapping apps have been proposed as a way to encourage environmentally beneficial actions.\footnote{A nudge is an “aspect of  choice architecture that alters people’s behaviour in a predictable way without forbidding any options or significantly changing economic incentives” (\href{https://psycnet.apa.org/record/2008-03730-000}{Thaler and Sunstein, 2008}).}  They can help close the gap between the desire to act more sustainably that many people express and their day to day behavior.\footnote{\label{note2}Siipi and Koi, 2021. https://philpapers.org/archive/SIITEO.pdf}  Some have argued that green nudges do not go far enough and that given the urgency of climate change the sustainable option should be the only one presented. Others argue that nudges infringe autonomy and so users should be explicitly asked which default they would prefer; others that nudges are only acceptable when the intention behind the nudge is transparently presented. For more context, see [\ref{note2}].
\end{enumerate}